{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IYRb_oZtzg4"
      },
      "source": [
        "# Learning Outcomes\n",
        "**In this tutotial you are going to learn:**\n",
        "\n",
        "  1: Basic NLP techniques to preprocess text\n",
        "\n",
        "  2: Differnt techniques to conver text into numerical form (CountVectorize, HashingTF, and TF-IDF)\n",
        "\n",
        "  3: Apply regression classifier on the text corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnGLhU2F_ezW"
      },
      "source": [
        "**Install spark and otehr necessary preliminiries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iYc02jqXRnA",
        "outputId": "5bbbb2ac-2b96-428e-f222-2d5f75a01998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=5025fc944abb55e7ff2fdb3964b564dd40e0916ee69f89f62af69e23a4b3bf57\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OE_cm03RYNCd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yVmM8rRaYXQw"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Spark_NLP\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFnCCrlrYcQ3",
        "outputId": "da4121df-6350-492f-f626-9549828d08bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIcKWpPPIyGf"
      },
      "source": [
        "#**Steps involve in NLP:**\n",
        "Text data can be very messy sometimes, and it needs careful attention to bring it to a stage where it can be used in the right way. There are multiple ways in which text data can be cleaned and refined. For example, regular expressions are very powerful when it comes to filtering, cleaning, and standardizing text data. However, regular expressions are not the focus area in this tutorial. Rather, we look at the steps to prepare text data in a form where we can fit a ML model on it. The five major steps involved in handling text data for ML modeling are:\n",
        "\n",
        "1.   Reading the corpus\n",
        "\n",
        "2.\tTokenization\n",
        "\n",
        "3.\tCleaning/stopword removal\n",
        "\n",
        "4.\tStemming\n",
        "\n",
        "5.\tConverting into numerical form\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5uQSWU9JfhE"
      },
      "source": [
        "**Creating Carpous **\n",
        "\n",
        "A corpus is known as an entire collection of text documents, for example, a collection of emails, messages, or user reviews. This group of individual text items is known as a **corpus**.\n",
        "We will use CSV file contain collection of text later in this tutorial. First we will do some basic pre-processing using text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVTtb_hWKd1N"
      },
      "source": [
        "##**1. Tokenization**\n",
        "\n",
        "The method of dividing the given text sequence/sentence or collection of words of a text document into individual words is known as tokenization. It removes the unnecessary characters such as punctuations. The final units post-tokenization are known as tokens. Let’s say we have the following text:\n",
        "\n",
        "**Input:** 'He really liked the London City. He is there for two more days'\n",
        "\n",
        "Tokenization would result in the following tokens.\n",
        "\n",
        "[He, really, liked, the, London, City, He, is, there, for, two, more, days]\n",
        "\n",
        "We end up with 13 tokens for the input text:\n",
        "\n",
        "Let us see how we can do tokenization using PySpark.\n",
        "The first step is to create a dataframe that has text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_LmIjBpKnOG",
        "outputId": "7038e21c-e4d1-4a74-ea0c-6cf1ba1bbfe8"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o288.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 21) (10.210.78.234 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#create data frame\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df2\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mcreateDataFrame([(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI really liked this movie wathing this movie was an unforgetable experience\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      4\u001b[0m                          (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI would recommend this movie to my friends\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      5\u001b[0m                          (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie was alright but acting was horrible\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      6\u001b[0m                          (\u001b[38;5;241m4\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI am never watching that movie ever again\u001b[39m\u001b[38;5;124m'\u001b[39m)],\n\u001b[0;32m      7\u001b[0m                         [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 10\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# When False is passed as the second argument, it means that long strings will not be truncated.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\jaree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:972\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    965\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    966\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         },\n\u001b[0;32m    970\u001b[0m     )\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\jaree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\jaree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Users\\jaree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o288.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 21) (10.210.78.234 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
          ]
        }
      ],
      "source": [
        "#create data frame\n",
        "\n",
        "df2=spark.createDataFrame([(1,'I really liked this movie wathing this movie was an unforgetable experience'),\n",
        "                         (2,'I would recommend this movie to my friends'),\n",
        "                         (3,'movie was alright but acting was horrible'),\n",
        "                         (4,'I am never watching that movie ever again')],\n",
        "                        ['user_id','review'])\n",
        "\n",
        "\n",
        "df2.show(4,False)\n",
        "\n",
        "# When False is passed as the second argument, it means that long strings will not be truncated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JkGbXwyMILa"
      },
      "source": [
        "In this Dataframe, we have four sentences for tokenization. The next step is to import Tokenizer from the **Spark library**. We must then pass the input column and name the output column after tokenization. We use the transform function to apply tokenization to the review column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf85VzyVMN7e",
        "outputId": "a4719def-5cfe-4ac3-d219-bfe732fd81d2"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o181.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (10.210.78.234 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenization\u001b[38;5;241m=\u001b[39mTokenizer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m,outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenized_df2\u001b[38;5;241m=\u001b[39mtokenization\u001b[38;5;241m.\u001b[39mtransform(df2)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtokenized_df2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jaree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:972\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    965\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    966\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         },\n\u001b[0;32m    970\u001b[0m     )\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\jaree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\jaree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Users\\jaree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o181.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (10.210.78.234 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "tokenization=Tokenizer(inputCol='review',outputCol='tokens')\n",
        "tokenized_df2=tokenization.transform(df2)\n",
        "tokenized_df2.show(4,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vcLAubpwewG"
      },
      "source": [
        "We get a new column named tokens that contains the tokens for each sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ZMZQ9BQkW7"
      },
      "source": [
        "## **2. Stopword Removal**\n",
        "\n",
        "As you can observe, the tokens column contains very common words such as “this,” “the,” “to,” “was,” “that,” etc. These words are known as stopwords, and they seem to add very little value to the analysis. If they are to be used in analysis, it increases the computation overheads without adding any value or insight. Hence, it’s preferred to drop these stopwords from the tokens. In PySpark, we use **StopWordsRemover** to remove the stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlD-4f_PQxjY",
        "outputId": "007088dc-4b7e-489d-8580-c014b67b36f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
            "|user_id|tokens                                                                                  |refined_tokens                                                  |\n",
            "+-------+----------------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
            "|1      |[i, really, liked, this, movie, wathing, this, movie, was, an, unforgetable, experience]|[really, liked, movie, wathing, movie, unforgetable, experience]|\n",
            "|2      |[i, would, recommend, this, movie, to, my, friends]                                     |[recommend, movie, friends]                                     |\n",
            "|3      |[movie, was, alright, but, acting, was, horrible]                                       |[movie, alright, acting, horrible]                              |\n",
            "|4      |[i, am, never, watching, that, movie, ever, again]                                      |[never, watching, movie, ever]                                  |\n",
            "+-------+----------------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
        "#We then pass the tokens as the input column and name the output column as refined tokens.\n",
        "refined_df2=stopword_removal.transform(tokenized_df2)\n",
        "refined_df2.select(['user_id','tokens','refined_tokens']).show(4,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyG4WwXT0PUh"
      },
      "source": [
        "##**3. Stemming/Lemmatization**\n",
        "\n",
        "Lemmatization is a natural language processing technique that aims to reduce words to their base or dictionary form, known as the lemma. The base form is often referred to as the **root word or the canonical form**. Lemmatization takes into account the context and part of speech (POS) of a word in order to produce the most appropriate base form.\n",
        "\n",
        "The main goal of lemmatization is to normalize words so that different inflected forms of a word are treated as a single item. For example, the lemma of \"running\" would be \"run,\" and the lemma of \"played\" would be \"play.\" By reducing words to their base forms, lemmatization helps to eliminate redundancy and improve the accuracy and efficiency of text analysis tasks.\n",
        "\n",
        "Lemmatization is different from stemming, another popular text normalization technique. While stemming simply chops off prefixes or suffixes from words to derive the root form, lemmatization considers the vocabulary and morphological analysis of words to determine the appropriate base form.\n",
        "\n",
        "In natural language processing workflows, lemmatization is often used in tasks such as information retrieval, text classification, topic modeling, and sentiment analysis. It helps to consolidate the representation of words, enabling better understanding and analysis of text data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRNQcCcb5LRj",
        "outputId": "8706aaef-388d-43bc-a109-c62195009675"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#We will use nltk library to define and use lemmatizer.\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLie-Vr94ctm",
        "outputId": "4c775685-e63b-4e0d-c71c-acca81518554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------+\n",
            "|user_id|review                                                                     |tokens                                                                                  |refined_tokens                                                  |lemmatized_words                                                |\n",
            "+-------+---------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------+\n",
            "|1      |I really liked this movie wathing this movie was an unforgetable experience|[i, really, liked, this, movie, wathing, this, movie, was, an, unforgetable, experience]|[really, liked, movie, wathing, movie, unforgetable, experience]|[really, liked, movie, wathing, movie, unforgetable, experience]|\n",
            "|2      |I would recommend this movie to my friends                                 |[i, would, recommend, this, movie, to, my, friends]                                     |[recommend, movie, friends]                                     |[recommend, movie, friend]                                      |\n",
            "|3      |movie was alright but acting was horrible                                  |[movie, was, alright, but, acting, was, horrible]                                       |[movie, alright, acting, horrible]                              |[movie, alright, acting, horrible]                              |\n",
            "|4      |I am never watching that movie ever again                                  |[i, am, never, watching, that, movie, ever, again]                                      |[never, watching, movie, ever]                                  |[never, watching, movie, ever]                                  |\n",
            "+-------+---------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Define lemmatization function using nltk library\n",
        "def lemmatize_text(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmas\n",
        "\n",
        "# Register lemmatization function as a UDF\n",
        "lemmatize_udf = udf(lemmatize_text, ArrayType(StringType()))\n",
        "\n",
        "# Apply lemmatization using the UDF\n",
        "refined_df2 = refined_df2.withColumn(\"lemmatized_words\", lemmatize_udf(\"refined_tokens\"))\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "refined_df2.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv-5DNGAymxU"
      },
      "source": [
        "Note for the second review 'friends' became 'friend'.  When you have bigger text corpous you will observe more changes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2-BVhMjUH5G"
      },
      "source": [
        "##**4.Converting into numerical form/ Feature Engineering**\n",
        "\n",
        "\n",
        "Now that we have the key tokens created from the text data, we now need a mechanism to convert the tokens into a **numerical form** because we know for a fact that a Machine Learning algorithm works on numerical data. Text data is generally unstructured and varies in its length.There are many techniques and we will use few in this tutorial:\n",
        "\n",
        "\n",
        "**Bag of Words**\n",
        "\n",
        "Bag of words allows to convert the text data into numerical vector form by considering the occurrence of the words in text documents, for example:\n",
        "\n",
        "Doc 1: The best thing in life is to travel\n",
        "\n",
        "Doc 2: Travel is the best medicine\n",
        "\n",
        "Doc 3: One should travel more often\n",
        "\n",
        "**vocabulary**\n",
        "\n",
        "The list of unique words appearing in all the documents is known as **vocabulary** . In the above example, we have a total of 13 unique words appearing that are part of the vocab as follow.\n",
        "\n",
        "[The, Best, thing, in, life, is, to, travel, medicine, one, should, more,often]\n",
        "\n",
        "Any of these three documents can be represented by this vector of fixed size 13 using a Boolean value (1 or 0):\n",
        "\n",
        "**Doc 1:**\n",
        "The Best thing in life is to travel medicine one should more often\n",
        "\n",
        "1   1    1     1  1    1  1  1      0        0    0     0    0\n",
        "\n",
        "**Doc 2:**\n",
        "\n",
        "The Best thing in life is to travel medicine one should more often\n",
        "\n",
        "0   1    0     0   0    0  0   1      1       0    0     0    0\n",
        "\n",
        "\n",
        "**Doc 3:**\n",
        "\n",
        "The Best thing in life is to travel medicine one should more often\n",
        "\n",
        "0   0    0     0  0    0   0   1      0        0    1     1    1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goAhSjA7XUV2"
      },
      "source": [
        "Bag of words does not consider the order of words in the document and the semantic meaning of the word and hence is the most baseline method to represent the text data in numerical form.\n",
        "There are other ways by which we can convert the textual data into numerical form, which are covered in the following section.\n",
        "We will use PySpark to go through each one of these methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3eO1zLDXbaC"
      },
      "source": [
        "###**CountVectorizer**\n",
        "\n",
        "In bag of words, we saw the representation of the occurrence of a word by simply **1 or 0** and did not consider the frequency of the word.\n",
        "\n",
        "The count vectorizer instead takes the **total count of the tokens** appearing in the particular document. We will use the same text documents that we created earlier during tokenization. We first import CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBcNWfr-Xr_R",
        "outputId": "4c422a2f-7f4f-4457-d95f-918f11d96c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------------------------------------------------------------+---------------------------------------------+\n",
            "|user_id|lemmatized_words                                                |features                                     |\n",
            "+-------+----------------------------------------------------------------+---------------------------------------------+\n",
            "|1      |[really, liked, movie, wathing, movie, unforgetable, experience]|(14,[0,2,3,5,8,12],[2.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "|2      |[recommend, movie, friend]                                      |(14,[0,4,11],[1.0,1.0,1.0])                  |\n",
            "|3      |[movie, alright, acting, horrible]                              |(14,[0,1,9,13],[1.0,1.0,1.0,1.0])            |\n",
            "|4      |[never, watching, movie, ever]                                  |(14,[0,6,7,10],[1.0,1.0,1.0,1.0])            |\n",
            "+-------+----------------------------------------------------------------+---------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "count_vec=CountVectorizer(inputCol='lemmatized_words',outputCol='features')\n",
        "cv_df2=count_vec.fit(refined_df2).transform(refined_df2)\n",
        "cv_df2.select(['user_id','lemmatized_words','features']).show(4,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocOET11UZUaW"
      },
      "source": [
        "CountVectorizer builds a vocabulary of all the unique words in the corpus and assigns each word an index. The dimensionality of the feature vectors is equal to the size of the vocabulary, which can be very large for large corpora.\n",
        "As we can observe, each row is represented as a dense vector. It shows that the vector length/dimension is 14 and the first sentence contains **6** word/values of the **vocabulary** at the **0,2,3,5, 8 and 12th** indexes.\n",
        "\n",
        "Note correspond to the 0th index count is 2. It should be value for the word 'movie' in the first review as count of 'movie' in this sentence is 2.\n",
        "You can use vocabulary function to see number of unique words in the vocabulary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rITcfqv3as25",
        "outputId": "f9efc847-fa40-4107-cbd5-0a6a191e332e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['movie',\n",
              " 'horrible',\n",
              " 'experience',\n",
              " 'liked',\n",
              " 'recommend',\n",
              " 'really',\n",
              " 'never',\n",
              " 'watching',\n",
              " 'unforgetable',\n",
              " 'alright',\n",
              " 'ever',\n",
              " 'friend',\n",
              " 'wathing',\n",
              " 'acting']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#To validate the vocabulary of the count vectorizer, we can simply use the vocabulary function:\n",
        "vocabulary=count_vec.fit(refined_df2).vocabulary\n",
        "vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXiIGVYXbDnk"
      },
      "source": [
        "Hence, the vocabulary size for the preceding sentences is 11; and if you look at the features carefully, they are like the input feature vector we have been using for Machine Learning in PySpark.\n",
        "\n",
        "The drawback of using the CountVectorizer method is that it doesn’t consider the **co-occurrences of words in other documents.**\n",
        "\n",
        "In simple terms, the words appearing often would have larger impact on the feature vector. Hence, another approach to convert text data into numerical form is **TF-IDF** (Term Frequency – Inverse Document Frequency)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2hwsegMbShr"
      },
      "source": [
        "###**TF-IDF**\n",
        "\n",
        "This method tries to normalize the frequency of token occurrence based on other documents. The whole idea is to give more weight to the token if appearing high number of times in the same document but penalize if it is appearing higher number of times in other documents as well. This indicates that the token is common across the corpus and is not as important as its frequency in the current document indicates.\n",
        "\n",
        "**Term Frequency:** Score based on the frequency of the word in the current document\n",
        "\n",
        "**Inverse Document Frequency:** Scoring based on the number of documents that contain the current word\n",
        "\n",
        "Now, we create features based on TF-IDF in PySpark using the same refined df Dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vXAgMabbxJH",
        "outputId": "ae27a125-fa85-4c28-a766-0c52e160073a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------------------------------------------------------------+---------------------------------------------------------------------------+\n",
            "|user_id|lemmatized_words                                                |tf_features                                                                |\n",
            "+-------+----------------------------------------------------------------+---------------------------------------------------------------------------+\n",
            "|1      |[really, liked, movie, wathing, movie, unforgetable, experience]|(262144,[55875,97005,99172,210223,229264,259272],[1.0,1.0,1.0,2.0,1.0,1.0])|\n",
            "|2      |[recommend, movie, friend]                                      |(262144,[68228,74520,210223],[1.0,1.0,1.0])                                |\n",
            "|3      |[movie, alright, acting, horrible]                              |(262144,[95685,171118,210223,236263],[1.0,1.0,1.0,1.0])                    |\n",
            "|4      |[never, watching, movie, ever]                                  |(262144,[63139,113673,203802,210223],[1.0,1.0,1.0,1.0])                    |\n",
            "+-------+----------------------------------------------------------------+---------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF,IDF\n",
        "hashing_vec2=HashingTF(inputCol='lemmatized_words',outputCol='tf_features')\n",
        "hashing_df2=hashing_vec2.transform(refined_df2)\n",
        "hashing_df2.select(['user_id','lemmatized_words','tf_features']).show(4,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9whkWplNUchR"
      },
      "source": [
        "**HashingTF:** It uses a hashing function to map features to indices in a fixed-size feature space. The number of features is determined by the size of the feature space. This means that the dimensionality of the feature vectors is fixed and independent of the actual vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNGpJGnTcs0r",
        "outputId": "94d6b4ce-2095-403f-b2e9-7a8a18658974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|user_id|tf_idf_features                                                                                                                                       |\n",
            "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1      |(262144,[55875,97005,99172,210223,229264,259272],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.0,0.9162907318741551,0.9162907318741551])|\n",
            "|2      |(262144,[68228,74520,210223],[0.9162907318741551,0.9162907318741551,0.0])                                                                             |\n",
            "|3      |(262144,[95685,171118,210223,236263],[0.9162907318741551,0.9162907318741551,0.0,0.9162907318741551])                                                  |\n",
            "|4      |(262144,[63139,113673,203802,210223],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.0])                                                  |\n",
            "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tf_idf_vec2=IDF(inputCol='tf_features',outputCol='tf_idf_features')\n",
        "tf_idf_df2=tf_idf_vec2.fit(hashing_df2).transform(hashing_df2)\n",
        "tf_idf_df2.select(['user_id','tf_idf_features']).show(4,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIEcyUnr86-M"
      },
      "source": [
        "Now we have required features and we can use machine learning models to do something insightful on the preprocessed text corpous. For example, text classification, clustering and so on.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d5LYVOTeS9Y"
      },
      "source": [
        "#**Text Classification Using Machine Learning**\n",
        "\n",
        "Now that we understand the steps involved in dealing with text processing and feature vectorization, we can build a text classification model and use it for predictions on text data. The dataset that we are going to use is a sample of the open source movie lens reviews data, and we’re going to predict the sentiment of the given review (positive or negative). Let’s start with reading the text data first and creating a Spark Dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iEnywlDheyVh"
      },
      "outputs": [],
      "source": [
        " # Read CSV file into a DataFrame\n",
        " #download the CSV file came along with this tutorial\n",
        " #save in ggogle drive and change path according tou your dierectory structure\n",
        " text_df= spark.read.csv('Movie_reviews.csv',inferSchema=True, header =True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z70jrzXIbP0t"
      },
      "source": [
        "**Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Explore data to understand it by looking it number of rows, columns, data types, etc.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgcz0Ez4ezWo",
        "outputId": "a7ab7808-623e-45b6-89df-10e42a1bf441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Review: string (nullable = true)\n",
            " |-- Sentiment: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#check number of columns and their data type\n",
        "text_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oVBqMuAUyxV"
      },
      "source": [
        "As we can see, the Sentiment column is StringType, and we will need to convert it into an integer or float type going forward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVfh3Wl5gAHF",
        "outputId": "227279a4-9e58-464e-d56b-07a547f947af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------+---------+\n",
            "|Review                                                                  |Sentiment|\n",
            "+------------------------------------------------------------------------+---------+\n",
            "|The Da Vinci Code book is just awesome.                                 |1        |\n",
            "|this was the first clive cussler i've ever read, but even books like Rel|1        |\n",
            "|i liked the Da Vinci Code a lot.                                        |1        |\n",
            "|i liked the Da Vinci Code a lot.                                        |1        |\n",
            "|I liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.|1        |\n",
            "|that's not even an exaggeration ) and at midnight we went to Wal-Mart to|1        |\n",
            "|I loved the Da Vinci Code, but now I want something better and different|1        |\n",
            "|i thought da vinci code was great, same with kite runner.               |1        |\n",
            "|The Da Vinci Code is actually a good movie...                           |1        |\n",
            "|I thought the Da Vinci Code was a pretty good book.                     |1        |\n",
            "+------------------------------------------------------------------------+---------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Show the first few rows of the DataFrame\n",
        "text_df.show(10, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJEcYo_hYFHp",
        "outputId": "7925a1de-5766-43d6-a59e-fac60847befb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7087, 2)\n"
          ]
        }
      ],
      "source": [
        "print((text_df.count(), len(text_df.columns)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeWvJ6loVAiG"
      },
      "source": [
        "We have close to 7k records, out of which some might not be labeled properly. Hence, we filter only those records that are labeled correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up08WPUPfhRQ",
        "outputId": "88f6a8e8-1ca9-4fd6-cbf2-4b1cf690b5ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6990"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_df=text_df.filter(((text_df.Sentiment =='1') | (text_df.Sentiment =='0')))\n",
        "text_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucu7GUzNVi31"
      },
      "source": [
        "Some of the records got filtered out, and we are now left with 6990 records for the analysis. The next step is to validate the number of reviews for each class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sC36TLwYlRg",
        "outputId": "3ffe9598-6155-4f37-bd17-1e39524c5951"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_df.select('Sentiment').distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4A9vWWdgy7D",
        "outputId": "74705b87-1807-4d0f-b573-bf3056eb7abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "|        0| 3081|\n",
            "|        1| 3909|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#We have no unlabeled tweets in the dataset. The next step is to validate the number of reviews for each class:\n",
        "text_df.groupBy('Sentiment').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QSHd1vIhR9t"
      },
      "source": [
        "We are dealing with a **balanced dataset**  here as both classes have an almost similar number of reviews.\n",
        "\n",
        "As a next step, we create a new integer-type **Label column** and **drop the original Sentiment column,** which was string type:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcnXON_QWiT2",
        "outputId": "417bf1b7-9d53-4579-e4cb-f856a170a662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------+-----+\n",
            "|Review                                                                 |Label|\n",
            "+-----------------------------------------------------------------------+-----+\n",
            "|Brokeback Mountain is a really depressing movie...                     |0.0  |\n",
            "|Brokeback Mountain was an AWESOME movie.                               |1.0  |\n",
            "|Da Vinci Code sucks.                                                   |0.0  |\n",
            "|Da Vinci Code sucks be...                                              |0.0  |\n",
            "|Brokeback Mountain was so awesome.                                     |1.0  |\n",
            "|Is it just me, or does Harry Potter suck?...                           |0.0  |\n",
            "|we're gonna like watch Mission Impossible or Hoot.(                    |1.0  |\n",
            "|we're gonna like watch Mission Impossible or Hoot.(                    |1.0  |\n",
            "|da vinci code was an awesome movie...                                  |1.0  |\n",
            "|i love being a sentry for mission impossible and a station for bonkers.|1.0  |\n",
            "+-----------------------------------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text_df=text_df.withColumn(\"Label\",text_df.Sentiment.cast('float')).drop('Sentiment')\n",
        "from pyspark.sql.functions import rand\n",
        "text_df.orderBy(rand()).show(10,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS02n1QGYix2"
      },
      "source": [
        "We include an additional column that captures the length of the review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loputXn7iXv4",
        "outputId": "f42bcd40-0c16-43f7-b71c-a55e582595aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------+-----+------+\n",
            "|Review                                                             |Label|length|\n",
            "+-------------------------------------------------------------------+-----+------+\n",
            "|The Da Vinci Code is awesome..                                     |1.0  |30    |\n",
            "|Da Vinci Code sucked..                                             |0.0  |22    |\n",
            "|* brokeback mountain is an awesome movie..                         |1.0  |42    |\n",
            "|da vinci code sucks...                                             |0.0  |22    |\n",
            "|I hate Harry Potter.                                               |0.0  |20    |\n",
            "|by the way, the Da Vinci Code sucked, just letting you know...     |0.0  |62    |\n",
            "|dudeee i LOVED brokeback mountain!!!!                              |1.0  |37    |\n",
            "|Ok brokeback mountain is such a horrible movie.                    |0.0  |47    |\n",
            "|I love Harry Potter.                                               |1.0  |20    |\n",
            "|the people who are worth it know how much i love the da vinci code.|1.0  |67    |\n",
            "+-------------------------------------------------------------------+-----+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import length\n",
        "from pyspark.sql.functions import rand\n",
        "text_df=text_df.withColumn('length',length(text_df['Review']))\n",
        "text_df.orderBy(rand()).show(10,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btLvy0c4i_VT",
        "outputId": "6f6b7fe0-babd-41fb-f8c1-290968550fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------+\n",
            "|Label|      avg(Length)|\n",
            "+-----+-----------------+\n",
            "|  1.0|47.61882834484523|\n",
            "|  0.0|50.95845504706264|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text_df.groupBy('Label').agg({'Length':'mean'}).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaQP2eX3jHL-"
      },
      "source": [
        "There is no major difference between the average length of the positive and negative review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp42tWmpcD7S"
      },
      "source": [
        "**Tokenization and Stopwords Removing**\n",
        "\n",
        "The next step is to start the **tokenization process and remove stopwords:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8vO7zdwjQQQ",
        "outputId": "068594c0-1400-4198-91f1-b82429130c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------+-----+------+----------------------------------------------------------------------------------------+\n",
            "|Review                                                                  |Label|length|tokens                                                                                  |\n",
            "+------------------------------------------------------------------------+-----+------+----------------------------------------------------------------------------------------+\n",
            "|The Da Vinci Code book is just awesome.                                 |1.0  |39    |[the, da, vinci, code, book, is, just, awesome.]                                        |\n",
            "|this was the first clive cussler i've ever read, but even books like Rel|1.0  |72    |[this, was, the, first, clive, cussler, i've, ever, read,, but, even, books, like, rel] |\n",
            "|i liked the Da Vinci Code a lot.                                        |1.0  |32    |[i, liked, the, da, vinci, code, a, lot.]                                               |\n",
            "|i liked the Da Vinci Code a lot.                                        |1.0  |32    |[i, liked, the, da, vinci, code, a, lot.]                                               |\n",
            "|I liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.|1.0  |72    |[i, liked, the, da, vinci, code, but, it, ultimatly, didn't, seem, to, hold, it's, own.]|\n",
            "|that's not even an exaggeration ) and at midnight we went to Wal-Mart to|1.0  |72    |[that's, not, even, an, exaggeration, ), and, at, midnight, we, went, to, wal-mart, to] |\n",
            "|I loved the Da Vinci Code, but now I want something better and different|1.0  |72    |[i, loved, the, da, vinci, code,, but, now, i, want, something, better, and, different] |\n",
            "|i thought da vinci code was great, same with kite runner.               |1.0  |57    |[i, thought, da, vinci, code, was, great,, same, with, kite, runner.]                   |\n",
            "|The Da Vinci Code is actually a good movie...                           |1.0  |45    |[the, da, vinci, code, is, actually, a, good, movie...]                                 |\n",
            "|I thought the Da Vinci Code was a pretty good book.                     |1.0  |51    |[i, thought, the, da, vinci, code, was, a, pretty, good, book.]                         |\n",
            "+------------------------------------------------------------------------+-----+------+----------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenization=Tokenizer(inputCol='Review',outputCol='tokens')\n",
        "tokenized_df=tokenization.transform(text_df)\n",
        "tokenized_df.show(10, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSL6Psa66Ftx",
        "outputId": "6041fdce-be62-411f-fcfc-8fc4b6fa0658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------+-----+------+-----------------------------------------------------------------------------------------+\n",
            "|Review                                                                  |Label|length|tokens                                                                                   |\n",
            "+------------------------------------------------------------------------+-----+------+-----------------------------------------------------------------------------------------+\n",
            "|The Da Vinci Code book is just awesome.                                 |1.0  |39    |[the, da, vinci, code, book, is, just, awesome]                                          |\n",
            "|this was the first clive cussler i've ever read, but even books like Rel|1.0  |72    |[this, was, the, first, clive, cussler, i, ve, ever, read, but, even, books, like, rel]  |\n",
            "|i liked the Da Vinci Code a lot.                                        |1.0  |32    |[i, liked, the, da, vinci, code, a, lot]                                                 |\n",
            "|i liked the Da Vinci Code a lot.                                        |1.0  |32    |[i, liked, the, da, vinci, code, a, lot]                                                 |\n",
            "|I liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.|1.0  |72    |[i, liked, the, da, vinci, code, but, it, ultimatly, didn, t, seem, to, hold, it, s, own]|\n",
            "|that's not even an exaggeration ) and at midnight we went to Wal-Mart to|1.0  |72    |[that, s, not, even, an, exaggeration, and, at, midnight, we, went, to, wal, mart, to]   |\n",
            "|I loved the Da Vinci Code, but now I want something better and different|1.0  |72    |[i, loved, the, da, vinci, code, but, now, i, want, something, better, and, different]   |\n",
            "|i thought da vinci code was great, same with kite runner.               |1.0  |57    |[i, thought, da, vinci, code, was, great, same, with, kite, runner]                      |\n",
            "|The Da Vinci Code is actually a good movie...                           |1.0  |45    |[the, da, vinci, code, is, actually, a, good, movie]                                     |\n",
            "|I thought the Da Vinci Code was a pretty good book.                     |1.0  |51    |[i, thought, the, da, vinci, code, was, a, pretty, good, book]                           |\n",
            "+------------------------------------------------------------------------+-----+------+-----------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        " #OR use RegexTokenizer to only keep the words and remove the special characters such as ')',etc\n",
        " # Remove hashtags and special characters from the review column\n",
        " from pyspark.ml.feature import RegexTokenizer\n",
        " tokenization=RegexTokenizer(inputCol= 'Review' , outputCol= 'tokens', pattern= '\\\\W')\n",
        " tokenized_df=tokenization.transform(text_df)\n",
        "tokenized_df.show(10, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viagifxzjfkC",
        "outputId": "46080285-2ecf-42a9-ddb9-d95dc54c36aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------+-----+------+-----------------------------------------------------------------------------------------+---------------------------------------------------------------+\n",
            "|Review                                                                  |Label|length|tokens                                                                                   |refined_tokens                                                 |\n",
            "+------------------------------------------------------------------------+-----+------+-----------------------------------------------------------------------------------------+---------------------------------------------------------------+\n",
            "|The Da Vinci Code book is just awesome.                                 |1.0  |39    |[the, da, vinci, code, book, is, just, awesome]                                          |[da, vinci, code, book, awesome]                               |\n",
            "|this was the first clive cussler i've ever read, but even books like Rel|1.0  |72    |[this, was, the, first, clive, cussler, i, ve, ever, read, but, even, books, like, rel]  |[first, clive, cussler, ve, ever, read, even, books, like, rel]|\n",
            "|i liked the Da Vinci Code a lot.                                        |1.0  |32    |[i, liked, the, da, vinci, code, a, lot]                                                 |[liked, da, vinci, code, lot]                                  |\n",
            "|i liked the Da Vinci Code a lot.                                        |1.0  |32    |[i, liked, the, da, vinci, code, a, lot]                                                 |[liked, da, vinci, code, lot]                                  |\n",
            "|I liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.|1.0  |72    |[i, liked, the, da, vinci, code, but, it, ultimatly, didn, t, seem, to, hold, it, s, own]|[liked, da, vinci, code, ultimatly, didn, seem, hold]          |\n",
            "|that's not even an exaggeration ) and at midnight we went to Wal-Mart to|1.0  |72    |[that, s, not, even, an, exaggeration, and, at, midnight, we, went, to, wal, mart, to]   |[even, exaggeration, midnight, went, wal, mart]                |\n",
            "|I loved the Da Vinci Code, but now I want something better and different|1.0  |72    |[i, loved, the, da, vinci, code, but, now, i, want, something, better, and, different]   |[loved, da, vinci, code, want, something, better, different]   |\n",
            "|i thought da vinci code was great, same with kite runner.               |1.0  |57    |[i, thought, da, vinci, code, was, great, same, with, kite, runner]                      |[thought, da, vinci, code, great, kite, runner]                |\n",
            "|The Da Vinci Code is actually a good movie...                           |1.0  |45    |[the, da, vinci, code, is, actually, a, good, movie]                                     |[da, vinci, code, actually, good, movie]                       |\n",
            "|I thought the Da Vinci Code was a pretty good book.                     |1.0  |51    |[i, thought, the, da, vinci, code, was, a, pretty, good, book]                           |[thought, da, vinci, code, pretty, good, book]                 |\n",
            "+------------------------------------------------------------------------+-----+------+-----------------------------------------------------------------------------------------+---------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Stop word removal\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
        "refined_text_df=stopword_removal.transform(tokenized_df)\n",
        "refined_text_df.show(10, False)\n",
        "#note the difeerence between tokens and refined_tokens to understand how stop word removal works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIQyqcmud9vv"
      },
      "source": [
        "Since we are now dealing with tokens only instead of an entire review, it would make more sense to capture a number of tokens in each review rather than using the length of the review. We create another column (token count) that gives the number of tokens in each row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VZ5z4LR4ebUf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ag6xUSoVe1Qk"
      },
      "outputs": [],
      "source": [
        "len_udf = udf(lambda s: len(s), IntegerType())\n",
        "#This is a user defined function (udf), the lambda function takes a single argument s,\n",
        "#which is a string. It calculates the length of the string s using the built-in len() function\n",
        "#return an integer value for length of the string.\n",
        "refined_text_df = refined_text_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCsV7JdJe9X0",
        "outputId": "8d94151a-2682-475f-a004-a70769f25845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
            "|              Review|Label|length|              tokens|      refined_tokens|token_count|\n",
            "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
            "|looks amazingly f...|  1.0|    72|[looks, amazingly...|[looks, amazingly...|          8|\n",
            "|Combining the opi...|  0.0|    71|[combining, the, ...|[combining, opini...|          9|\n",
            "|Also, Da Vinci Co...|  0.0|    35|[also, da, vinci,...|[also, da, vinci,...|          5|\n",
            "|After some frenzi...|  1.0|    72|[after, some, fre...|[frenzied, phone,...|          7|\n",
            "|The Da Vinci Code...|  1.0|    41|[the, da, vinci, ...|[da, vinci, code,...|          5|\n",
            "|I hate Harry Pott...|  0.0|    71|[i, hate, harry, ...|[hate, harry, pot...|          7|\n",
            "|Then snuck into B...|  0.0|    72|[then, snuck, int...|[snuck, brokeback...|          5|\n",
            "|Harry Potter drag...|  0.0|    69|[harry, potter, d...|[harry, potter, d...|          8|\n",
            "|Then snuck into B...|  0.0|    72|[then, snuck, int...|[snuck, brokeback...|          5|\n",
            "|I want to be here...|  1.0|    72|[i, want, to, be,...|[want, love, harr...|          7|\n",
            "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "refined_text_df.orderBy(rand()).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGrFjip6py98"
      },
      "source": [
        "**Converting into numerical form/ Feature Engineering**\n",
        "\n",
        "Now that we have the refined tokens after stopword removal, we can use any of the preceding approaches to convert text into numerical features. In this case, we use CountVectorizer for feature vectorization for the Machine Learning model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIsGywxVp2cW",
        "outputId": "6aca1ea8-ec20-4e0b-80b6-df7b1b18dcf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------+----------------------------------------------------------------------------------------+-----+\n",
            "|refined_tokens                                                 |features                                                                                |Label|\n",
            "+---------------------------------------------------------------+----------------------------------------------------------------------------------------+-----+\n",
            "|[da, vinci, code, book, awesome]                               |(1644,[0,1,2,8,167],[1.0,1.0,1.0,1.0,1.0])                                              |1.0  |\n",
            "|[first, clive, cussler, ve, ever, read, even, books, like, rel]|(1644,[11,41,42,178,180,206,214,806,990,1434],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|1.0  |\n",
            "|[liked, da, vinci, code, lot]                                  |(1644,[0,1,2,43,182],[1.0,1.0,1.0,1.0,1.0])                                             |1.0  |\n",
            "|[liked, da, vinci, code, lot]                                  |(1644,[0,1,2,43,182],[1.0,1.0,1.0,1.0,1.0])                                             |1.0  |\n",
            "|[liked, da, vinci, code, ultimatly, didn, seem, hold]          |(1644,[0,1,2,43,204,611,1068,1114],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |1.0  |\n",
            "|[even, exaggeration, midnight, went, wal, mart]                |(1644,[39,178,639,992,1416,1510],[1.0,1.0,1.0,1.0,1.0,1.0])                             |1.0  |\n",
            "|[loved, da, vinci, code, want, something, better, different]   |(1644,[0,1,2,22,23,62,317,390],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                       |1.0  |\n",
            "|[thought, da, vinci, code, great, kite, runner]                |(1644,[0,1,2,56,177,619,777],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                             |1.0  |\n",
            "|[da, vinci, code, actually, good, movie]                       |(1644,[0,1,2,12,172,201],[1.0,1.0,1.0,1.0,1.0,1.0])                                     |1.0  |\n",
            "|[thought, da, vinci, code, pretty, good, book]                 |(1644,[0,1,2,167,172,174,177],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                            |1.0  |\n",
            "+---------------------------------------------------------------+----------------------------------------------------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
        "cv_text_df=count_vec.fit(refined_text_df).transform(refined_text_df)\n",
        "cv_text_df.select(['refined_tokens','features','Label']).show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCfNTHrwavOg"
      },
      "source": [
        "(1644,[0,1,2,8,167],[1.0,1.0,1.0,1.0,1.0])\n",
        "\n",
        "Vocabulary size/dimension is 1644.\n",
        "Each row is represented as a vector. It shows that the vector length is 1644 and the first review contains five values at the 0,1,2,8,and 167th indexes.All other values shoud be zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gDH8qU7hcIy",
        "outputId": "c6c8126c-1551-4d4d-f4da-7d14a7842cb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['da',\n",
              " 'vinci',\n",
              " 'code',\n",
              " 'harry',\n",
              " 'brokeback',\n",
              " 'mountain',\n",
              " 'potter',\n",
              " 'love',\n",
              " 'awesome',\n",
              " 'mission',\n",
              " 'impossible',\n",
              " 'like',\n",
              " 'movie',\n",
              " 'hate',\n",
              " 'sucked',\n",
              " 'sucks',\n",
              " 'much',\n",
              " 'really',\n",
              " 'movies',\n",
              " 'know',\n",
              " 'suck',\n",
              " '3',\n",
              " 'want',\n",
              " 'loved',\n",
              " 'think',\n",
              " 'one',\n",
              " 'stupid',\n",
              " 'depressing',\n",
              " '2',\n",
              " 'reading',\n",
              " 'horrible',\n",
              " 'fucking',\n",
              " 'terrible',\n",
              " 'oh',\n",
              " 'right',\n",
              " 'left',\n",
              " 'ok',\n",
              " 'felicia',\n",
              " 'beautiful',\n",
              " 'went',\n",
              " 'saw',\n",
              " 'read',\n",
              " 'first',\n",
              " 'liked',\n",
              " 'tom',\n",
              " 'absolutely',\n",
              " 'way',\n",
              " 'heard',\n",
              " 'big',\n",
              " 'time',\n",
              " 'film',\n",
              " 'said',\n",
              " 'going',\n",
              " 'boring',\n",
              " 'series',\n",
              " 'watch',\n",
              " 'great',\n",
              " 'people',\n",
              " 'yeah',\n",
              " 'man',\n",
              " 're',\n",
              " 'got',\n",
              " 'better',\n",
              " 'watching',\n",
              " 'story',\n",
              " 'things',\n",
              " 'b',\n",
              " 'last',\n",
              " 'friday',\n",
              " 'gay',\n",
              " 'person',\n",
              " 'wait',\n",
              " 'rocks',\n",
              " 'says',\n",
              " 'cool',\n",
              " 'anyone',\n",
              " 'excellent',\n",
              " 'always',\n",
              " 'knows',\n",
              " 'anyway',\n",
              " 'mom',\n",
              " 'friends',\n",
              " 'review',\n",
              " 'worth',\n",
              " 'daniel',\n",
              " 'opinion',\n",
              " 'dad',\n",
              " 'either',\n",
              " 'care',\n",
              " 'thats',\n",
              " 'never',\n",
              " 'p',\n",
              " 'guy',\n",
              " 'stand',\n",
              " 'luv',\n",
              " 'gonna',\n",
              " 'table',\n",
              " 'awards',\n",
              " 'hates',\n",
              " 'head',\n",
              " 'turned',\n",
              " 'type',\n",
              " 'wanted',\n",
              " 'crazy',\n",
              " 'hill',\n",
              " 'place',\n",
              " 'needs',\n",
              " 'hat',\n",
              " 'make',\n",
              " 'snuck',\n",
              " 'start',\n",
              " 'past',\n",
              " 'soo',\n",
              " 'mtv',\n",
              " 'trousers',\n",
              " 'lubb',\n",
              " 'slap',\n",
              " 'hella',\n",
              " 'leah',\n",
              " 'reminded',\n",
              " 'dudeee',\n",
              " 'hung',\n",
              " 'wel',\n",
              " 'helped',\n",
              " 'kelsie',\n",
              " 'draco',\n",
              " 'deep',\n",
              " 'mat',\n",
              " 'letting',\n",
              " 'sentry',\n",
              " 'gin',\n",
              " 'acne',\n",
              " '5',\n",
              " 'wotshisface',\n",
              " 'homosexuality',\n",
              " 'hips',\n",
              " 'escapades',\n",
              " 'grabs',\n",
              " 'malfoy',\n",
              " 'reality',\n",
              " 'kirsten',\n",
              " 'laughe',\n",
              " 'bye',\n",
              " 'retarted',\n",
              " 'kate',\n",
              " 'quiz',\n",
              " 'combining',\n",
              " 'differently',\n",
              " 'bonkers',\n",
              " 'desperately',\n",
              " 'sit',\n",
              " 'profound',\n",
              " 'insanely',\n",
              " 'outshines',\n",
              " 'hoot',\n",
              " 'stars',\n",
              " 'cleaning',\n",
              " 'dragged',\n",
              " 'gary',\n",
              " 'silent',\n",
              " 'coz',\n",
              " 'bobbypin',\n",
              " 'keys',\n",
              " 'zen',\n",
              " 'station',\n",
              " 'rig',\n",
              " 'tha',\n",
              " 'book',\n",
              " 'hated',\n",
              " 'also',\n",
              " 'm',\n",
              " 'iii',\n",
              " 'good',\n",
              " 'see',\n",
              " 'pretty',\n",
              " 'though',\n",
              " 'ass',\n",
              " 'thought',\n",
              " 'even',\n",
              " 'say',\n",
              " 'books',\n",
              " 'evil',\n",
              " 'lot',\n",
              " 'well',\n",
              " 'still',\n",
              " 'go',\n",
              " 'balls',\n",
              " 'get',\n",
              " 'amazing',\n",
              " 'watched',\n",
              " 'kinda',\n",
              " 'already',\n",
              " 'miss',\n",
              " 'cruise',\n",
              " 'awful',\n",
              " 'd',\n",
              " 'lol',\n",
              " 'made',\n",
              " 'seen',\n",
              " 'may',\n",
              " 'thing',\n",
              " 'actually',\n",
              " 'three',\n",
              " 'second',\n",
              " 'didn',\n",
              " 'best',\n",
              " 'ever',\n",
              " 'talk',\n",
              " 'tell',\n",
              " 'us',\n",
              " 'crash',\n",
              " 'enjoy',\n",
              " 'making',\n",
              " 'two',\n",
              " 've',\n",
              " 'll',\n",
              " 'won',\n",
              " 'hey',\n",
              " 'enjoyed',\n",
              " 'im',\n",
              " 'try',\n",
              " 'new',\n",
              " 'c',\n",
              " 'action',\n",
              " 'theme',\n",
              " 'totally',\n",
              " 'looks',\n",
              " 'everyone',\n",
              " 'x',\n",
              " 'sad',\n",
              " 'okay',\n",
              " 'back',\n",
              " 'real',\n",
              " 'far',\n",
              " 'since',\n",
              " 'night',\n",
              " 'fact',\n",
              " 'god',\n",
              " 'bogus',\n",
              " 'mother',\n",
              " 'probably',\n",
              " 'th',\n",
              " 'lord',\n",
              " 'finished',\n",
              " 'little',\n",
              " 'luck',\n",
              " 'hear',\n",
              " 'inaccurate',\n",
              " 'doesn',\n",
              " 'course',\n",
              " 'anything',\n",
              " 'school',\n",
              " 'wrong',\n",
              " 'haven',\n",
              " 'almost',\n",
              " 'apparently',\n",
              " 'personally',\n",
              " 'men',\n",
              " 'song',\n",
              " 'every',\n",
              " 'long',\n",
              " 'sick',\n",
              " 'shit',\n",
              " 'glad',\n",
              " 'sure',\n",
              " 'n',\n",
              " 'quite',\n",
              " 'used',\n",
              " 'times',\n",
              " 'demons',\n",
              " 'mean',\n",
              " 'freaking',\n",
              " 'angels',\n",
              " 'thinking',\n",
              " 'talking',\n",
              " 'end',\n",
              " 'aweso',\n",
              " 'fun',\n",
              " '10',\n",
              " 'seeing',\n",
              " 'saying',\n",
              " 'omg',\n",
              " 'kick',\n",
              " 'tonight',\n",
              " 'crap',\n",
              " 'music',\n",
              " 'stories',\n",
              " 'w',\n",
              " 'rings',\n",
              " 'bit',\n",
              " 'move',\n",
              " 'show',\n",
              " 'telling',\n",
              " 'officially',\n",
              " 'shitty',\n",
              " 'suc',\n",
              " 'sa',\n",
              " 'fan',\n",
              " 'wa',\n",
              " 'life',\n",
              " 'yes',\n",
              " 'else',\n",
              " 'georgia',\n",
              " 'interesting',\n",
              " 'news',\n",
              " 'wish',\n",
              " 'gotta',\n",
              " 'must',\n",
              " 'crappy',\n",
              " 'someone',\n",
              " 'stinks',\n",
              " 'stayed',\n",
              " 'part',\n",
              " 'majorly',\n",
              " 'donkey',\n",
              " 'except',\n",
              " 'win',\n",
              " 'something',\n",
              " 'came',\n",
              " 'btw',\n",
              " 'fat',\n",
              " 'erm',\n",
              " 'dumb',\n",
              " 'v',\n",
              " 'let',\n",
              " 'religious',\n",
              " 'gun',\n",
              " 'thank',\n",
              " 'score',\n",
              " 'picture',\n",
              " 'sorry',\n",
              " 'hard',\n",
              " 'given',\n",
              " 'day',\n",
              " 'shows',\n",
              " 'fire',\n",
              " 'death',\n",
              " 'stuff',\n",
              " 'ma',\n",
              " 'wanna',\n",
              " 'whole',\n",
              " 'case',\n",
              " 'board',\n",
              " 'week',\n",
              " 'lost',\n",
              " 'year',\n",
              " 'tautou',\n",
              " 'yet',\n",
              " 'hell',\n",
              " 'agree',\n",
              " 'give',\n",
              " 'brokebac',\n",
              " 'nearly',\n",
              " 'goblet',\n",
              " 'pot',\n",
              " 'might',\n",
              " 'gorgeous',\n",
              " 'f',\n",
              " 'fo',\n",
              " 'critics',\n",
              " 'least',\n",
              " 'top',\n",
              " 'stupi',\n",
              " 'wondering',\n",
              " 'disliked',\n",
              " 'deal',\n",
              " 'none',\n",
              " 'around',\n",
              " 'anyways',\n",
              " 'l',\n",
              " 'fandom',\n",
              " 'stone',\n",
              " 'incredibly',\n",
              " 'gift',\n",
              " 'rather',\n",
              " 'come',\n",
              " 'half',\n",
              " 'sexy',\n",
              " 'conclusion',\n",
              " 'sucking',\n",
              " 'besides',\n",
              " 'audrey',\n",
              " 'whi',\n",
              " 'jake',\n",
              " 'hope',\n",
              " 'k',\n",
              " 'take',\n",
              " 'comment',\n",
              " 'impossibl',\n",
              " 'looking',\n",
              " 'different',\n",
              " 'bad',\n",
              " 'aaron',\n",
              " '33',\n",
              " 'lov',\n",
              " 'lah',\n",
              " 'ya',\n",
              " '_',\n",
              " 'dictate',\n",
              " 'point',\n",
              " 'stop',\n",
              " 'loves',\n",
              " 'write',\n",
              " 'blood',\n",
              " 'co',\n",
              " 'trailers',\n",
              " 'tho',\n",
              " 'goth',\n",
              " 'aka',\n",
              " 'cried',\n",
              " 'strangely',\n",
              " 'cars',\n",
              " '4',\n",
              " 'gla',\n",
              " 'decent',\n",
              " 'classes',\n",
              " 'cod',\n",
              " 'forgotten',\n",
              " 'u',\n",
              " 'figures',\n",
              " 'fell',\n",
              " 'whenever',\n",
              " 'reply',\n",
              " 'john',\n",
              " 'dislike',\n",
              " 'asian',\n",
              " 'level',\n",
              " 'j',\n",
              " 'world',\n",
              " 'guys',\n",
              " 'la',\n",
              " 'ha',\n",
              " 'days',\n",
              " 'adorable',\n",
              " 'although',\n",
              " 'happy',\n",
              " 'conquering',\n",
              " 'major',\n",
              " 'girl',\n",
              " 'cocktail',\n",
              " 'expected',\n",
              " 'films',\n",
              " 'funny',\n",
              " 'sucke',\n",
              " 'icons',\n",
              " 'everything',\n",
              " 'idea',\n",
              " 'yea',\n",
              " 'normal',\n",
              " 'eragon',\n",
              " 'mountai',\n",
              " 'exquisite',\n",
              " 'sorcerer',\n",
              " 'nothing',\n",
              " 'lousy',\n",
              " 'theaters',\n",
              " 'phillip',\n",
              " 'form',\n",
              " '6th',\n",
              " 'funniest',\n",
              " 'lame',\n",
              " 'record',\n",
              " 'cowboys',\n",
              " 'gaither',\n",
              " 'loathe',\n",
              " 'piece',\n",
              " 'bul',\n",
              " 'hogwarts',\n",
              " 'drive',\n",
              " 'jesus',\n",
              " 'academy',\n",
              " 'fault',\n",
              " 'play',\n",
              " 'girls',\n",
              " 'ultimate',\n",
              " 'damn',\n",
              " 'blame',\n",
              " 'many',\n",
              " 'choice',\n",
              " 'thousand',\n",
              " 'awes',\n",
              " 'marcia',\n",
              " 'nc',\n",
              " 'immediately',\n",
              " 'scar',\n",
              " 'quizzes',\n",
              " 'short',\n",
              " 'everybody',\n",
              " 'club',\n",
              " 'truly',\n",
              " 'definitely',\n",
              " 'hating',\n",
              " 'house',\n",
              " 'character',\n",
              " 'passion',\n",
              " 'food',\n",
              " 'wesley',\n",
              " 'tale',\n",
              " 'sing',\n",
              " 'showing',\n",
              " 'rowling',\n",
              " 'playing',\n",
              " 'weekend',\n",
              " 'prin',\n",
              " 'told',\n",
              " 'makes',\n",
              " 'felt',\n",
              " 'unbelievably',\n",
              " 'kids',\n",
              " 'ang',\n",
              " 'home',\n",
              " 'generally',\n",
              " 'party',\n",
              " 'cuz',\n",
              " 'acting',\n",
              " 'need',\n",
              " 'friend',\n",
              " 'imp',\n",
              " 'g',\n",
              " 'decided',\n",
              " 'wasn',\n",
              " 'laid',\n",
              " 'franchise',\n",
              " 'y',\n",
              " 'flick',\n",
              " 'murderball',\n",
              " 'fanfiction',\n",
              " 'comes',\n",
              " 'tickets',\n",
              " 'future',\n",
              " 'finally',\n",
              " 'cold',\n",
              " 'ending',\n",
              " 'lie',\n",
              " 'fireworks',\n",
              " 'christian',\n",
              " 'draw',\n",
              " 'friendships',\n",
              " 'ask',\n",
              " 'tv',\n",
              " 'heart',\n",
              " 'cry',\n",
              " 'whether',\n",
              " 'opened',\n",
              " 'oscar',\n",
              " 'suppose',\n",
              " 'al',\n",
              " 'marvel',\n",
              " 'yesterday',\n",
              " 'novel',\n",
              " 'mi3',\n",
              " 'mouth',\n",
              " 'pages',\n",
              " 'watson',\n",
              " 'instead',\n",
              " 'emma',\n",
              " 'dance',\n",
              " 'po',\n",
              " 'super',\n",
              " 'christmas',\n",
              " 'sometimes',\n",
              " 'definately',\n",
              " 'idk',\n",
              " 'sooo',\n",
              " 'sounds',\n",
              " 'fabulous',\n",
              " 'mention',\n",
              " 'couple',\n",
              " 'feel',\n",
              " 'ran',\n",
              " 'woo',\n",
              " 'hero',\n",
              " 'ps',\n",
              " 'attempt',\n",
              " 'please',\n",
              " 'dragons',\n",
              " 'word',\n",
              " 'update',\n",
              " 'turn',\n",
              " 'education',\n",
              " 'hoover',\n",
              " 'lee',\n",
              " 'especially',\n",
              " 'haunt',\n",
              " 'royally',\n",
              " 'next',\n",
              " 'lin',\n",
              " 'rant',\n",
              " 'awesomest',\n",
              " 'kind',\n",
              " 'sivullinen',\n",
              " '1',\n",
              " 'use',\n",
              " 'thinks',\n",
              " 'compared',\n",
              " 'mrs',\n",
              " 'hollywood',\n",
              " 'dick',\n",
              " 'knew',\n",
              " 'played',\n",
              " 'abou',\n",
              " 'favourite',\n",
              " 'kid',\n",
              " 'vampire',\n",
              " 'ban',\n",
              " 'requested',\n",
              " 'vi',\n",
              " 'fantasy',\n",
              " 'cringe',\n",
              " 'work',\n",
              " 'chris',\n",
              " 'hold',\n",
              " 'ide',\n",
              " 'worthless',\n",
              " 'shout',\n",
              " 'raises',\n",
              " 'mormon',\n",
              " 'gavin',\n",
              " 'nothin',\n",
              " 'runner',\n",
              " 'hoffman',\n",
              " 'friggin',\n",
              " 'fowl',\n",
              " 'depth',\n",
              " 'supporting',\n",
              " 'actio',\n",
              " 'amazes',\n",
              " 'ignore',\n",
              " 'crusade',\n",
              " 'plus',\n",
              " 'shut',\n",
              " 'gosh',\n",
              " 'omen',\n",
              " 'rosie',\n",
              " 'classic',\n",
              " 'stitch',\n",
              " 'tan',\n",
              " 'pull',\n",
              " 'spontaneously',\n",
              " 'midnight',\n",
              " 'tragic',\n",
              " 'inappropriate',\n",
              " 'teac',\n",
              " 'recently',\n",
              " 'planned',\n",
              " 'guts',\n",
              " 'rep',\n",
              " 'found',\n",
              " 'small',\n",
              " 'screens',\n",
              " 'comprehend',\n",
              " 'latter',\n",
              " 'straight',\n",
              " 'idiot',\n",
              " 'spanish',\n",
              " 'meat',\n",
              " 'af',\n",
              " 'futile',\n",
              " 'invisible',\n",
              " 'giving',\n",
              " 'mo',\n",
              " 'project',\n",
              " 'main',\n",
              " 'denial',\n",
              " 'liberal',\n",
              " 'pirated',\n",
              " 'months',\n",
              " 'awkward',\n",
              " 'settin',\n",
              " 'irrespective',\n",
              " 'often',\n",
              " 'refusing',\n",
              " 'joan',\n",
              " 'hollywoord',\n",
              " 'field',\n",
              " 'disappointed',\n",
              " 'eve',\n",
              " 'absolute',\n",
              " 'deluded',\n",
              " 'freagin',\n",
              " 'jessica',\n",
              " 'arse',\n",
              " 'meeting',\n",
              " 'precious',\n",
              " 'lines',\n",
              " 'bough',\n",
              " 'orig',\n",
              " 'heather',\n",
              " 'undercover',\n",
              " 'community',\n",
              " 'mcphee',\n",
              " 'apart',\n",
              " 'sucky',\n",
              " 'haunted',\n",
              " 'hot',\n",
              " 'threw',\n",
              " 'believabl',\n",
              " 'picnic',\n",
              " 'baby',\n",
              " 'react',\n",
              " 'quaintly',\n",
              " 'apolo',\n",
              " 'scarf',\n",
              " 'decides',\n",
              " 'ive',\n",
              " 'mindedness',\n",
              " 'wiccans',\n",
              " 'lotr',\n",
              " 'lynne',\n",
              " 'quip',\n",
              " '1st',\n",
              " 'scientology',\n",
              " 'quick',\n",
              " 'obviously',\n",
              " 'everytime',\n",
              " 'roommate',\n",
              " 'twice',\n",
              " 'dumbest',\n",
              " 'springer',\n",
              " 'staying',\n",
              " 'muahahaaha',\n",
              " 'vault',\n",
              " 'ser',\n",
              " 'open',\n",
              " 'listens',\n",
              " 'surprised',\n",
              " 'involved',\n",
              " 'intellectual',\n",
              " 'taking',\n",
              " 'buy',\n",
              " 'archive',\n",
              " 'radio',\n",
              " 'acoustic',\n",
              " 'spoke',\n",
              " 'devastate',\n",
              " 'othe',\n",
              " 'chunnel',\n",
              " 'rode',\n",
              " 'hands',\n",
              " 'rps',\n",
              " 'clit',\n",
              " 'controversy',\n",
              " 'dont',\n",
              " 'gladly',\n",
              " 'emotes',\n",
              " 'theatan',\n",
              " 'defensive',\n",
              " 'bible',\n",
              " 'genres',\n",
              " 'robe',\n",
              " 'correct',\n",
              " 'invisibility',\n",
              " 'festivities',\n",
              " 'brilliant',\n",
              " 'potte',\n",
              " 'free',\n",
              " 'matters',\n",
              " 'magical',\n",
              " 'tonite',\n",
              " 'reader',\n",
              " 'reaction',\n",
              " 'bootlegged',\n",
              " 'photography',\n",
              " 'entire',\n",
              " 'movi',\n",
              " 'chicken',\n",
              " 'decide',\n",
              " 'portuguese',\n",
              " 'texts',\n",
              " 'bitch',\n",
              " 'wh',\n",
              " 'weeeellllllll',\n",
              " 'cloak',\n",
              " 'dearly',\n",
              " 'forget',\n",
              " 'lama',\n",
              " 'images',\n",
              " 'kite',\n",
              " 'dream',\n",
              " '16',\n",
              " 'touching',\n",
              " 'ab',\n",
              " 'larry',\n",
              " 'spi',\n",
              " 'condemnation',\n",
              " '200',\n",
              " 'lesson',\n",
              " 'due',\n",
              " 'rented',\n",
              " 'ne',\n",
              " 'pirates',\n",
              " 'useless',\n",
              " 'catch',\n",
              " 'superman',\n",
              " 'feeling',\n",
              " 'pc',\n",
              " 'hahash',\n",
              " 'jon',\n",
              " 'cant',\n",
              " 'criticized',\n",
              " 'yuh',\n",
              " 'poem',\n",
              " 'witha',\n",
              " 'seems',\n",
              " 'sis',\n",
              " 'fuckers',\n",
              " 'rel',\n",
              " 'screwed',\n",
              " 'conversation',\n",
              " 'die',\n",
              " 'grey',\n",
              " 'side',\n",
              " 'decaying',\n",
              " 'col',\n",
              " 'rocked',\n",
              " 'itz',\n",
              " 'france',\n",
              " 'posted',\n",
              " 'calling',\n",
              " 'author',\n",
              " 'believe',\n",
              " 'anime',\n",
              " 'requiem',\n",
              " 'phoenix',\n",
              " 'german',\n",
              " 'search',\n",
              " 'done',\n",
              " 'nice',\n",
              " 'figure',\n",
              " 'proud',\n",
              " 'worse',\n",
              " 'fits',\n",
              " 'animated',\n",
              " 'spectacularly',\n",
              " 'shes',\n",
              " 'ra',\n",
              " 'reason',\n",
              " 'involving',\n",
              " 'fix',\n",
              " 'rereading',\n",
              " 'spec',\n",
              " 'experience',\n",
              " 'caribbean',\n",
              " 'professors',\n",
              " 'mentioned',\n",
              " 'ew',\n",
              " '3333',\n",
              " 'weiners',\n",
              " 'overlooking',\n",
              " 'tomkat',\n",
              " 'theres',\n",
              " 'stu',\n",
              " 'close',\n",
              " 'selfish',\n",
              " 'talked',\n",
              " 'jelly',\n",
              " 'aimee',\n",
              " 'ago',\n",
              " 'overslept',\n",
              " 'shitties',\n",
              " 'match',\n",
              " 'burbank',\n",
              " 'sisters',\n",
              " 'tired',\n",
              " 'becuase',\n",
              " 'jack',\n",
              " 'dress',\n",
              " '10pm',\n",
              " '70',\n",
              " 'rice',\n",
              " 'interview',\n",
              " 'shraddha',\n",
              " 'add',\n",
              " 'mang',\n",
              " 'adversity',\n",
              " 'packed',\n",
              " 'pi',\n",
              " 'thirdly',\n",
              " 'style',\n",
              " 'task',\n",
              " 'presented',\n",
              " 'england',\n",
              " 'pop',\n",
              " 'mocking',\n",
              " 'hoff',\n",
              " 'budget',\n",
              " 'riding',\n",
              " 'grea',\n",
              " 'suckin',\n",
              " 'astonishingly',\n",
              " 'bolsters',\n",
              " 'credit',\n",
              " 'hahahaha',\n",
              " 'murdered',\n",
              " 'fuc',\n",
              " 'vice',\n",
              " 'slash',\n",
              " 'hugged',\n",
              " 'inten',\n",
              " 'missi',\n",
              " 'kill',\n",
              " 'total',\n",
              " 'halloween',\n",
              " 'fanfic',\n",
              " 'bee',\n",
              " 'children',\n",
              " 'kicking',\n",
              " 'section',\n",
              " 'messiah',\n",
              " 'benefit',\n",
              " 'shittiest',\n",
              " 'ignorant',\n",
              " 'mindless',\n",
              " 'anyhow',\n",
              " 'etc',\n",
              " 'honestly',\n",
              " 'blog',\n",
              " 'rachel',\n",
              " 'cos',\n",
              " 'tc',\n",
              " 'brooke',\n",
              " 'dorks',\n",
              " 'reference',\n",
              " 'working',\n",
              " 'planning',\n",
              " 'reasons',\n",
              " 'disappointing',\n",
              " 'infuser',\n",
              " 'haha',\n",
              " 'television',\n",
              " 'wiccanism',\n",
              " 'following',\n",
              " 'crack',\n",
              " 'boycott',\n",
              " 'natur',\n",
              " 'coming',\n",
              " 'headmistress',\n",
              " 'hilarious',\n",
              " 'geek',\n",
              " 'wow',\n",
              " 'featured',\n",
              " 'preview',\n",
              " 'offense',\n",
              " 'seeking',\n",
              " 'media',\n",
              " 'personaly',\n",
              " 'beans',\n",
              " 'excersizing',\n",
              " 'perhaps',\n",
              " 'gettin',\n",
              " 'creature',\n",
              " 'afterschool',\n",
              " 'madre',\n",
              " 'tragically',\n",
              " 'cake',\n",
              " 'melbourne',\n",
              " 'ri',\n",
              " 'spells',\n",
              " 'positions',\n",
              " 'simply',\n",
              " 'dungeons',\n",
              " 'aga',\n",
              " 'aside',\n",
              " 'mall',\n",
              " 'writes',\n",
              " 'success',\n",
              " 'grow',\n",
              " 'ot',\n",
              " 'warns',\n",
              " 'johnny',\n",
              " 'drove',\n",
              " 'wrote',\n",
              " 'interest',\n",
              " 'worst',\n",
              " 'canceled',\n",
              " 'matter',\n",
              " 'bullshit',\n",
              " 'decompo',\n",
              " 'conversations',\n",
              " 'questions',\n",
              " 'heath',\n",
              " 'banning',\n",
              " 'probable',\n",
              " 'shields',\n",
              " 'anus',\n",
              " 'scientologist',\n",
              " 'japenese',\n",
              " 'demeantor',\n",
              " 'unfortunate',\n",
              " 'twilight',\n",
              " 'cussler',\n",
              " 'boycotting',\n",
              " 'wal',\n",
              " 'along',\n",
              " 'pink',\n",
              " 'awe',\n",
              " 'illustrated',\n",
              " 'barry',\n",
              " 'adore',\n",
              " 'sixth',\n",
              " ...]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#explore vocabulary\n",
        "vocabulary=count_vec.fit(refined_text_df).vocabulary\n",
        "vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-C5AQ7FsA-h",
        "outputId": "54100b26-ecd4-4e53-d5a5-32492ff86294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            features|Label|\n",
            "+--------------------+-----+\n",
            "|(1644,[0,1,2,8,16...|  1.0|\n",
            "|(1644,[11,41,42,1...|  1.0|\n",
            "|(1644,[0,1,2,43,1...|  1.0|\n",
            "|(1644,[0,1,2,43,1...|  1.0|\n",
            "|(1644,[0,1,2,43,2...|  1.0|\n",
            "|(1644,[39,178,639...|  1.0|\n",
            "|(1644,[0,1,2,22,2...|  1.0|\n",
            "|(1644,[0,1,2,56,1...|  1.0|\n",
            "|(1644,[0,1,2,12,1...|  1.0|\n",
            "|(1644,[0,1,2,167,...|  1.0|\n",
            "|(1644,[0,1,2,18,2...|  1.0|\n",
            "|(1644,[0,1,2,167,...|  1.0|\n",
            "|(1644,[0,1,2,211,...|  1.0|\n",
            "|(1644,[0,1,2,17,1...|  1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|\n",
            "|(1644,[0,1,2,23],...|  1.0|\n",
            "|(1644,[0,1,2,38,2...|  1.0|\n",
            "|(1644,[0,1,2,8,16...|  1.0|\n",
            "|(1644,[0,1,2,200,...|  1.0|\n",
            "|(1644,[0,1,2,188,...|  1.0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now we have a feture vector for each row and ready for the next step to make prediction.\n",
        "#we only use feature vector and Lable to train any ML model, hence select these two columns from the data frame.\n",
        "model_text_df=cv_text_df.select(['features','Label'])\n",
        "model_text_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706gN3_jkIAT"
      },
      "source": [
        "Once we have the feature vector for each row, we can make use of VectorAssembler to create input features for the Machine Learning model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SniX453vsVLl",
        "outputId": "430d1e1e-d4d0-4f2e-af51-d1613bf8b736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- Label: float (nullable = true)\n",
            " |-- features_vec: vector (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "df_assembler = VectorAssembler(inputCols=['features'],outputCol='features_vec')\n",
        "model_text_df = df_assembler.transform(model_text_df)\n",
        "model_text_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q9pYiS-kWOZ",
        "outputId": "02ea9385-6e19-4ed0-dcb0-30a26fe58ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+--------------------+\n",
            "|            features|Label|        features_vec|\n",
            "+--------------------+-----+--------------------+\n",
            "|(1644,[0,1,2,8,16...|  1.0|(1644,[0,1,2,8,16...|\n",
            "|(1644,[11,41,42,1...|  1.0|(1644,[11,41,42,1...|\n",
            "|(1644,[0,1,2,43,1...|  1.0|(1644,[0,1,2,43,1...|\n",
            "|(1644,[0,1,2,43,1...|  1.0|(1644,[0,1,2,43,1...|\n",
            "|(1644,[0,1,2,43,2...|  1.0|(1644,[0,1,2,43,2...|\n",
            "|(1644,[39,178,639...|  1.0|(1644,[39,178,639...|\n",
            "|(1644,[0,1,2,22,2...|  1.0|(1644,[0,1,2,22,2...|\n",
            "|(1644,[0,1,2,56,1...|  1.0|(1644,[0,1,2,56,1...|\n",
            "|(1644,[0,1,2,12,1...|  1.0|(1644,[0,1,2,12,1...|\n",
            "|(1644,[0,1,2,167,...|  1.0|(1644,[0,1,2,167,...|\n",
            "|(1644,[0,1,2,18,2...|  1.0|(1644,[0,1,2,18,2...|\n",
            "|(1644,[0,1,2,167,...|  1.0|(1644,[0,1,2,167,...|\n",
            "|(1644,[0,1,2,211,...|  1.0|(1644,[0,1,2,211,...|\n",
            "|(1644,[0,1,2,17,1...|  1.0|(1644,[0,1,2,17,1...|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|\n",
            "|(1644,[0,1,2,23],...|  1.0|(1644,[0,1,2,23],...|\n",
            "|(1644,[0,1,2,38,2...|  1.0|(1644,[0,1,2,38,2...|\n",
            "|(1644,[0,1,2,8,16...|  1.0|(1644,[0,1,2,8,16...|\n",
            "|(1644,[0,1,2,200,...|  1.0|(1644,[0,1,2,200,...|\n",
            "|(1644,[0,1,2,188,...|  1.0|(1644,[0,1,2,188,...|\n",
            "+--------------------+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_text_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qtyAhGytdQy"
      },
      "source": [
        "**We can use any of the classification model on this data, but we proceed with training a logistic regression model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "MG6uWDpMtfbq"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "training_df,test_df=model_text_df.randomSplit([0.75,0.25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMu8Be2utsya",
        "outputId": "db728631-7688-462c-94f6-b764e455d9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|Label|count|\n",
            "+-----+-----+\n",
            "|  1.0| 2948|\n",
            "|  0.0| 2300|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#To verify the presence of enough records for both classes in train and test sets, we can apply the groupBy function on the Label column:\n",
        "training_df.groupBy('Label').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jP883HKt6-F",
        "outputId": "0d71f4a5-1455-48b4-8cfe-e59c69f57cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|Label|count|\n",
            "+-----+-----+\n",
            "|  1.0|  961|\n",
            "|  0.0|  781|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_df.groupBy('Label').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Sc5NFXvnuKyh"
      },
      "outputs": [],
      "source": [
        "log_reg=LogisticRegression(featuresCol='features_vec',labelCol='Label').fit(training_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQxuteJSlmiv"
      },
      "source": [
        "After training the model, we evaluate the performance of the model on the **test dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MCgMSu9lanK",
        "outputId": "b61107e7-a106-4836-eefd-30e55e631f66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "|            features|Label|        features_vec|       rawPrediction|         probability|prediction|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "|(1644,[0,1,2,7],[...|  1.0|(1644,[0,1,2,7],[...|[-23.163802820220...|[8.71140600991758...|       1.0|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results=log_reg.evaluate(test_df).predictions\n",
        "results.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4w5HrVlB3S6v"
      },
      "outputs": [],
      "source": [
        "#Evaluate the model performance using confusion matrix\n",
        "true_postives = results[(results.Label == 1) & (results.prediction == 1)].count()\n",
        "true_negatives = results[(results.Label == 0) & (results.prediction == 0)].count()\n",
        "false_positives = results[(results.Label == 0) & (results.prediction == 1)].count()\n",
        "false_negatives = results[(results.Label == 1) & (results.prediction == 0)].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTg9fhHEmjqO",
        "outputId": "8a6fd06b-3da8-4142-8fe7-9e4e7d8ee97d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TP= 957\n",
            "TN = 762\n",
            "FP = 19\n",
            "FN = 4\n"
          ]
        }
      ],
      "source": [
        "print ('TP=', true_postives)\n",
        "print('TN =', true_negatives)\n",
        "print('FP =', false_positives)\n",
        "print('FN =', false_negatives)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIiMOZFM3sVR",
        "outputId": "c6be4659-ba14-4ced-8536-8481a20cc38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9958376690946931\n"
          ]
        }
      ],
      "source": [
        "recall = float(true_postives)/(true_postives + false_negatives)\n",
        "print(recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiwTCgJt30mv",
        "outputId": "2e962440-cf7d-4bee-cf6b-87c0a52c9a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9805327868852459\n"
          ]
        }
      ],
      "source": [
        "precision = float(true_postives) / (true_postives + false_positives)\n",
        "print(precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQXfkJEO39_s",
        "outputId": "d206c228-c57d-413b-ffc5-49da528fc63c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.986796785304248\n"
          ]
        }
      ],
      "source": [
        "accuracy=float((true_postives+true_negatives) /(results.count()))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L64t_HaG1eJo"
      },
      "source": [
        "**What is Next?**\n",
        "We used CountVectorizer to create features for classification model. You can use TF-ID and Observe the difference in results using CountVectorizer and TF-IDF model.\n",
        "\n",
        "Use different classification models which you have learned earlier and identify the bestt model using different feature engineering techniques such as TF-IDF, CountVectorizer.\n",
        "\n",
        "**Next Lesson**\n",
        "\n",
        "In the next session we will continue with other text represenation techniques, such as word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLI01Dz_taI6",
        "outputId": "8ea3b7fb-bd34-45fe-e842-a4f65511ded9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/ML&Bigdata/CN7030-Feature_Extraction using_TF-IDF.ipynb to html\n",
            "[NbConvertApp] Writing 722895 bytes to /content/drive/MyDrive/ML&Bigdata/CN7030-Feature_Extraction using_TF-IDF.html\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbconvert --to html \"/content/drive/MyDrive/ML&Bigdata/CN7030-Feature_Extraction using_TF-IDF.ipynb\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
